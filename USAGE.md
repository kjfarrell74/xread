# XReader Usage Guide

XReader is a versatile CLI tool for scraping Twitter/X data via Nitter instances and generating detailed factual reports using AI models like Perplexity AI. This guide provides detailed instructions on how to use XReader in various modes and scenarios.

## Table of Contents

- [Basic Usage](#basic-usage)
- [Interactive Mode](#interactive-mode)
- [Command-Line Mode](#command-line-mode)
- [Running with run.sh](#running-with-runsh)
- [Output Data Structure](#output-data-structure)
- [Tips and Best Practices](#tips-and-best-practices)

## Basic Usage

XReader can be run in two primary modes: interactive mode for a user-friendly experience, and command-line mode for quick operations or scripting.

### Prerequisites

Ensure you have:
- Installed XReader and its dependencies as per the [README.md](README.md) installation instructions.
- Set up your Perplexity API key in a `.env` file or as an environment variable.

## Interactive Mode

To start XReader in interactive mode, simply run the script without arguments:

```bash
python xread.py
```

In interactive mode, you can:
- Enter Twitter/X URLs directly to scrape and analyze them.
- Use commands like `list`, `stats`, `delete`, and `reload_instructions` to manage your scraped data.

### Interactive Commands

- **`list`**: Displays a list of all scraped posts stored in the data directory.
- **`stats`**: Shows statistics about the scraped data, such as the total number of posts and recent activity.
- **`delete <status_id>`**: Removes a specific post from the data store by its status ID.
- **`reload_instructions`**: Reloads the instruction set, useful if instructions have been updated.
- **`exit` or `quit`**: Exits the interactive mode.

## Command-Line Mode

For quick operations or integration into scripts, use command-line mode by providing arguments directly:

### Scrape a Specific URL

```bash
python xread.py scrape <URL>
```

Replace `<URL>` with the Twitter/X or Nitter URL of the post you want to scrape and analyze. Example:

```bash
python xread.py scrape https://x.com/user/status/123456789
```

### List Saved Posts

```bash
python xread.py list
```

### Show Statistics

```bash
python xread.py stats
```

### Delete a Saved Post

```bash
python xread.py delete <status_id>
```

Replace `<status_id>` with the ID of the post to delete. Example:

```bash
python xread.py delete 123456789
```

## Running with run.sh

For convenience, especially when setting up a virtual environment or loading environment variables, use the provided `run.sh` script:

### Run in Interactive Mode

```bash
bash run.sh
```

### Run with a Specific URL

```bash
bash run.sh https://x.com/user/status/123456789
```

The `run.sh` script handles virtual environment setup, dependency installation, and environment variable loading from a `.env` file if present.

## Output Data Structure

XReader saves scraped and analyzed data in JSON format in the `scraped_data` directory (configurable via `DATA_DIR` environment variable). The structure includes:

- **index.json**: A summary index of all scraped posts for quick reference.
- **post_[STATUSID].json**: Detailed data for each post, including:
  - Main post content and metadata.
  - Replies to the main post.
  - AI-generated factual report (currently using Perplexity AI).
  - Enhanced metadata like normalized dates, media flags, and image descriptions generated by the centralized data enhancement module.

Example structure of a post JSON file:

```json
{
  "main_post": {
    "text": "Post content here",
    "date_iso": "2023-10-01T12:00:00+00:00",
    "images": [{"url": "image_url", "description": "Generated description"}],
    "has_images": true,
    "has_video": false,
    "has_links": true,
    "likes": 100,
    "retweets": 50,
    "replies_count": 20
  },
  "replies": [
    {
      "text": "Reply content",
      "date_iso": "2023-10-01T12:05:00+00:00"
    }
  ],
  "factual_context": ["Fact 1", "Fact 2"],
  "topic_tags": ["tag1", "tag2"],
  "scrape_meta": {
    "scraped_at": "2023-10-01T12:10:00+00:00",
    "source": "x.com",
    "scraper": "perplexity-to-json-v1"
  }
}
```

## Tips and Best Practices

- **API Key Security**: Never hardcode API keys in scripts or commit them to version control. Use a `.env` file or environment variables to keep them secure.
- **Nitter Instance Selection**: Choose a reliable Nitter instance for scraping. If one instance is slow or down, configure a different one via the `NITTER_INSTANCE` environment variable.
- **Rate Limiting**: Be mindful of rate limits on both the Nitter instance and Perplexity AI API. Avoid rapid successive requests to prevent being temporarily blocked.
- **Data Management**: Regularly review and clean up old data using the `list` and `delete` commands to manage storage space.
- **Debugging**: If scraping or report generation fails, check the `debug_output` directory for logs and failed HTML content to diagnose issues.

For further assistance or to report issues, refer to the [CONTRIBUTING.md](CONTRIBUTING.md) guide.
